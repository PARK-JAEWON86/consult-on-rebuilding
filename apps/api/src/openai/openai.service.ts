import { Injectable } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import OpenAI from 'openai';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

@Injectable()
export class OpenAIService {
  private client: OpenAI;
  private readonly model: string;
  private readonly maxContextMessages: number = 20;

  constructor(private configService: ConfigService) {
    const apiKey = configService.get<string>('OPENAI_API_KEY');

    // ğŸ” ë””ë²„ê¹…: API í‚¤ í™•ì¸
    console.log('[OpenAIService] Initializing...');
    console.log('[OpenAIService] API Key present:', !!apiKey);
    console.log('[OpenAIService] API Key prefix:', apiKey ? apiKey.substring(0, 20) + '...' : 'MISSING');

    if (!apiKey) {
      console.error('[OpenAIService] âŒ CRITICAL: OPENAI_API_KEY is not configured!');
      throw new Error('OPENAI_API_KEY environment variable is required');
    }

    this.client = new OpenAI({
      apiKey: apiKey,
    });
    this.model = configService.get<string>('OPENAI_MODEL', 'gpt-3.5-turbo');

    console.log('[OpenAIService] âœ… Initialized successfully with model:', this.model);
  }

  /**
   * GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… ì‘ë‹µ ìƒì„±
   */
  async generateChatResponse(
    messages: Array<{ type: 'user' | 'ai'; content: string }>,
    systemPrompt?: string
  ): Promise<{ content: string; tokenCount: number }> {
    try {
      // ë©”ì‹œì§€ í¬ë§·íŒ… (ìµœê·¼ Nê°œë§Œ í¬í•¨)
      const formattedMessages = this.formatMessages(messages, systemPrompt);

      console.log('[OpenAIService] Calling OpenAI API...');
      console.log('[OpenAIService] Model:', this.model);
      console.log('[OpenAIService] Message count:', formattedMessages.length);

      // OpenAI API í˜¸ì¶œ
      const completion = await this.client.chat.completions.create({
        model: this.model,
        messages: formattedMessages,
        temperature: 0.7,
        max_tokens: 2000,
      });

      console.log('[OpenAIService] âœ… API call successful');
      console.log('[OpenAIService] Tokens used:', completion.usage?.total_tokens);

      return {
        content: completion.choices[0].message.content || '',
        tokenCount: completion.usage?.total_tokens || 0,
      };
    } catch (error) {
      console.error('[OpenAIService] âŒ API call failed:', error);

      if (error instanceof OpenAI.APIError) {
        console.error('[OpenAIService] OpenAI API Error details:', {
          status: error.status,
          type: error.type,
          code: error.code,
          message: error.message
        });
        throw this.handleAPIError(error);
      }

      console.error('[OpenAIService] Unknown error:', error);
      throw new Error('AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (UX ê°œì„ ìš©)
   */
  async *generateStreamingResponse(
    messages: Array<{ type: 'user' | 'ai'; content: string }>,
    systemPrompt?: string
  ): AsyncGenerator<string, { totalTokens: number }, void> {
    const formattedMessages = this.formatMessages(messages, systemPrompt);

    const stream = await this.client.chat.completions.create({
      model: this.model,
      messages: formattedMessages,
      temperature: 0.7,
      max_tokens: 2000,
      stream: true,
    });

    let totalTokens = 0;
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || '';
      if (content) {
        yield content;
      }
      // ë§ˆì§€ë§‰ ì²­í¬ì—ì„œ í† í° ìˆ˜ í™•ì¸
      if (chunk.usage) {
        totalTokens = chunk.usage.total_tokens;
      }
    }

    return { totalTokens };
  }

  /**
   * ë©”ì‹œì§€ë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
   */
  private formatMessages(
    messages: Array<{ type: 'user' | 'ai'; content: string }>,
    systemPrompt?: string
  ): ChatMessage[] {
    const formatted: ChatMessage[] = [];

    // System message ì¶”ê°€
    if (systemPrompt) {
      formatted.push({
        role: 'system',
        content: systemPrompt,
      });
    }

    // ìµœê·¼ Nê°œ ë©”ì‹œì§€ë§Œ í¬í•¨ (í† í° ì œí•œ)
    const recentMessages = messages.slice(-this.maxContextMessages);

    // ë©”ì‹œì§€ ë³€í™˜
    for (const msg of recentMessages) {
      formatted.push({
        role: msg.type === 'user' ? 'user' : 'assistant',
        content: msg.content,
      });
    }

    return formatted;
  }

  /**
   * API ì—ëŸ¬ ì²˜ë¦¬
   */
  private handleAPIError(error: any): Error {
    if (error.status === 429) {
      return new Error('AI ì„œë¹„ìŠ¤ê°€ ì¼ì‹œì ìœ¼ë¡œ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    }
    if (error.status === 401) {
      return new Error('AI ì„œë¹„ìŠ¤ ì¸ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
    if (error.status === 500) {
      return new Error('AI ì„œë¹„ìŠ¤ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
    return new Error(`AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜: ${error.message}`);
  }

  /**
   * ì½˜í…ì¸  ì•ˆì „ì„± ê²€ì‚¬ (ì„ íƒì‚¬í•­)
   */
  async moderateContent(text: string): Promise<boolean> {
    try {
      const moderation = await this.client.moderations.create({
        input: text,
      });
      return moderation.results[0].flagged;
    } catch (error) {
      console.error('Moderation error:', error);
      return false;
    }
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜ (ìƒë‹´ì‚¬ ì—­í• )
   */
  getSystemPrompt(): string {
    return `ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì—­í• :
- ì‚¬ìš©ìì˜ ê³ ë¯¼ê³¼ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
- ê³µê°ì ì´ê³  ë”°ëœ»í•œ íƒœë„ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤

ì§€ì¹¨:
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•©ë‹ˆë‹¤
- ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤
- í•„ìš”ì‹œ ë‹¨ê³„ë³„ë¡œ í•´ê²° ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤
- ë¯¼ê°í•œ ì£¼ì œëŠ” ì‹ ì¤‘í•˜ê²Œ ë‹¤ë£¹ë‹ˆë‹¤
- ë²•ì /ì˜ë£Œì  ì¡°ì–¸ì´ í•„ìš”í•œ ê²½ìš° ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œìœ í•©ë‹ˆë‹¤

ê¸ˆì§€ì‚¬í•­:
- ë¶€ì ì ˆí•˜ê±°ë‚˜ ìœ í•´í•œ ë‚´ìš© ìƒì„± ê¸ˆì§€
- ê°œì¸ì •ë³´ ìš”êµ¬ ê¸ˆì§€
- ì˜ë£Œ/ë²•ë¥  ì§„ë‹¨ ê¸ˆì§€`;
  }
}
